---
title: "Regresion Logistica"
output: html_notebook
---

```{r, echo=TRUE, message=FALSE}
library(tidyverse)
library(broom)
library(ISLR)
library(GGally)
library(modelr)
library(pROC)
library(cowplot)
library(OneR)
library(rlang)
library(purrr)
library(caret)
```

La regresión logística es útil para problemas de predicción de clases. El problema que vamos a tratar de resolver es predecir si una persona va defaultear su deuda de tarjeta de crédito en base a ciertos predictores.

##Conjunto de datos

Este conjunto de datos proviene de la librería [ISLR](http://www-bcf.usc.edu/%7Egareth/ISL/)  (Introduction to Statistical Learning Using R) de James, Witten, Hastie y Tibshirani.

```{r}
default <- Default
glimpse(default)
```

Tiene 4 variables: 

* **default**: La clase que queremos predecir
* **student**: Binaria que indica si la persona es estudiante
* **balance**: Balance promedio que le queda a la persona luego de sus pagos mensuales
* **income**: Ingreso de la persona

##Exploratorias

Analicemos la distribución de la clase

```{r}
default %>% group_by(default) %>% summarise(numero_casos=n())
```

Vemos que estamos trabajando con un problema de clasificación con un claro desbalance de clase.

Realizamos un gráfico exploratorio completo para ver el comportamiento y las relaciones entre las variables. El color rojo designa a quienes no defaultean y el azul a los que sí.

```{r, warning=FALSE, message=FALSE}
ggpairs(default,mapping = aes(colour= default)) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) + theme_bw()
```

¿Qué pueden decir de la relación entre balance y default?
¿Y entre income y balance?
¿Cuáles parecen ser buenas variables para predecir la probabilidad de default de una persona?


### Limpieza

Para modelizar va a ser más necesario tener la variable default como numérica. 
Definimos la variable como {0,1} para los valores {"No","Yes"}

```{r}
default <- default%>% mutate(default= case_when(default=="No"~0,
                                                default=="Yes"~1))
```

## Problema

Queremos estimar $P(Default=Yes|X)=P(X)$ para cada individuo y partir de ello poder definir un punto de corte para predecir quienes son los que van a entrar en default.

### Regresión lineal

En este caso estamos modelando la probabilidad de la siguiente manera: 

$P(X)= \beta_0 + \sum\limits_{j=1}^p \beta_j X$

Veamos que tan bueno es el modelo lineal para esto, usando balance como predictor.

```{r}
test_mco <- default %>% 
              lm(formula = default~balance, data = .) 

```

```{r}
tdy <- test_mco %>% tidy()
tdy
test_mco %>% glance()
```

Ambos estimadores son significativos y el test de significatividad global del modelo también es significativo.
Veamos un gráfico de nuestro modelo

```{r, echo=FALSE}
ggplot(default, aes(balance, default)) + 
  geom_point(aes(color=factor(default))) +
  geom_abline(intercept = tdy$estimate[1], slope = tdy$estimate[2], color='forestgreen', size=2) + 
  labs(title="Modelo lineal simple") +
  lims(y=c(-1,2))+
  theme_bw()
```

Parece tener bastantes problemas para estimar la probabilidad de default de los individuos. Por ejemplo, vemos que hay varios individuos a los cuales les asigna una probabilidad negativa.

### Regresión logística

Para evitar estos problemas, usamos la **funcion logistica**

$P(X)= \frac{e^{\beta_0 + \sum\limits_{j=1}^p \beta_j X}}{1+e^{\beta_0 + \sum\limits_{j=1}^p \beta_j X}}$

El lado derecho se llama **expit**

Esta funcion acota el resultado entre 0 y 1, lo cual es mucho mas adecuado para modelar una probabilidad.

Luego de hacer algunas operaciones, podemos llegar a la expresion:

$\log {\frac{P(X)}{1-P(x)}}= \beta_0 + \sum\limits_{j=1}^p \beta_j X$

El lado izquierdo es el logaritmo de los **odds** y se llama **logit**

## Creación de fórmulas

Para aplicar la regresion logistica primero usamos la funcion _formulas_ del paquete **modelr** para crear un objeto que contiene todas las formulas que vamos a utilizar.

```{r}
logit_formulas <- formulas(.response = ~default, # único lado derecho de las formulas.
                         bal= ~balance, 
                         stud= ~student,  
                         inc= ~income,  
                         bal_stud=~balance+student, 
                         bal_inc=~balance+income, 
                         stud_inc=~student+income,  
                         full= ~balance + income + student  
                         )
```

## Modelos simples

Probamos los primeros tres modelos, aquellos que tienen un único predictor. Usamos la función _tidy_ para obtener los parámetros estimados para estos tres modelos.

```{r}
# get_Logit_coef <- function(formula, dataset) {
#   glm(formula=formula, family = 'binomial', data = dataset) %>% tidy()
# }
# 
# simple_logit = NULL
# 
# for (i in c(1:3)) {
#   formula= logit_formulas[[i]]
#   logit =get_Logit_coef(formula, default)
#   simple_logit = rbind(simple_logit, logit)
# }
# 
# simple_logit
```

```{r}
models <- data_frame(logit_formulas) %>%
  mutate( models = names(logit_formulas),
         expresion = paste(logit_formulas)) %>% 
  group_by(models,expresion) %>% 
    mutate(mod = map(logit_formulas, ~  glm(as.formula(.),family = 'binomial', data = default)),
           tidy = map(mod,tidy))

models %>% 
  filter(models %in% c('bal','stud','inc')) %>%
  unnest(tidy, .drop = TRUE) %>% 
  mutate(p.value=round(p.value,4))


```

¿Son significativos?
¿Qué interpretación pueden darle a estos valores?

##Modelo completo

Ahora probamos con un modelo que utiliza las tres predictoras

```{r}
models %>% 
  filter(models == "full") %>%
  unnest(tidy, .drop = TRUE) %>% 
  mutate(p.value=round(p.value,4))
```



```{r}

# glm_full <- glm(formula=logit_formulas$full, family = 'binomial', data = default) %>% tidy() %>% mutate(p.value=round(p.value,4))
# glm_full  

```

¿Que cambios hay respecto a los 3 modelos individuales previos?

##Evaluación de todos los modelos

Con `map()` agregamos la función `glance` para traernos información relevante para el diagnóstico del modelo.

Con unnest podemos ver la evaluación de cada modelo. Por último ordenamos las modelos por el deviance.

```{r}
models <- models %>% 
  mutate(glance = map(mod,glance))

models %>% 
  unnest(glance, .drop = TRUE) %>% 
  arrange(deviance)
```



```{r}
# evaluate_logit <- function(formula, dataset=default){
#   glm <- glm(formula=formula, family = 'binomial', data = dataset) %>% glance() %>% mutate(formula=paste(as.character(formula), collapse=" "))
#   return(glm)
# }
# 
# logit_eval = map_dfr(logit_formulas, evaluate_logit)
# 
# logit_eval %>% select(-c(df.null)) %>% arrange(deviance)

```

El modelo que utiliza las 3 variables es el que minimiza el deviance. 
Los 3 últimos modelos reducen muy poco el deviance respecto a la deviance nula.

## Graficos de evaluación

Realizamos los gráficos para el modelo completo y uno de los modelos con mayor deviance (student+income).

En este caso estamos:

1) agregando las predicciones con `augment` con el parámetro type="response" ¿Por qué hacemos esto? ¿Cuál es el valor por default de este parámetro? 
2) Armando las curvas ROC con `pROC`

```{r}
# 1) Creando el modelo
# 2) Aplicando la función __predict.glm__ con el parámetro type="response" ¿Por qué hacemos esto? ¿Cuál es el valor por default de este parámetro? 
# 3) Creamos la curva ROC con la librería **pROC**
```


```{r}
models <- models %>% 
  mutate(pred= map(mod,augment, type.predict = "response"))

models$pred[1]
```


```{r}
prediction_full <- models %>% 
  filter(models=="full") %>% 
  unnest(pred, .drop=TRUE)
roc_full <- roc(response=prediction_full$default, predictor=prediction_full$.fitted)

prediction_bad <- models %>% 
  filter(models=="stud_inc") %>% 
  unnest(pred, .drop=TRUE)

roc_bad <- roc(response=prediction_bad$default, predictor=prediction_bad$.fitted)


```


```{r}

# #Modelo full
# glm_full <- glm(formula=logit_formulas$full, family = 'binomial', data = default)
# default['predicted_full']= predict.glm(glm_full, default, type='response') 
# roc_full <- roc(response=default$default, predictor=default$predicted_full)
# #Modelo student+income
# glm_bad <- glm(formula=logit_formulas$stud_inc, family = 'binomial', data = default)
# default['predicted_bad']= predict.glm(glm_bad, default, type='response') 
# roc_bad <- roc(response=default$default, predictor=default$predicted_bad)


```

#### Violin plots

```{r}
violin_full=ggplot(prediction_full, aes(x=default, y=.fitted)) + 
  geom_violin(aes(fill=default), fill = "darkgreen", alpha=0.75) +
  theme_bw() +
  guides(fill=FALSE) +
  labs(title='Violin plot', subtitle='Modelo completo', y='Predicted probability')

violin_bad=ggplot(prediction_bad, aes(x=default, y=.fitted)) + 
  geom_violin(aes(fill=default), fill="darkred", alpha=0.75) + 
  theme_bw() +
  guides(fill=FALSE) +
  labs(title='Violin plot', subtitle='Modelo malo', y='Predicted probability')

plot_grid(violin_bad, violin_full)
```

¿Qué es lo que estamos viendo en ellos? (Especial atención al eje de ordenadas)

¿Cuál parece ser un punto de corte adecuado para cada modelo?

#### Curvas ROC

```{r}
ggroc(list(full=roc_full, bad=roc_bad), size=1) + geom_abline(slope = 1, intercept = 1, linetype='dashed') + theme_bw() + labs(title='Curvas ROC', color='Modelo')
```

#### Gráfico de Hosmer-Lemeshow

```{r, message=FALSE}

Hosmer_Lemeshow_plot <- function(dataset, predicted_column, class_column, bins, positive_value, color='forestgreen', nudge_x=0, nudge_y=0.05){
  "Realiza un grafico de Hosmer-Lemeshow para un dataset"
  
  "* dataset: conjunto de datos
   * predicted_column: columna con la probabilidad predicha
   * class_column: columna con la clase a predecir
   * possitive_value: valor de la clase a predecir
   * bins: cantidad de grupos del gráfico
   * color: color de los puntos
   * nudge_x: desplazamiento de la etiqueta en el eje x
   * nudge_y: desplazamiento de la etiqueta en el eje y"
  
  # Asignar los grupos a las observaciones de acuerdo a la probabilidad predicha
  dataset['group'] <- bin(dataset[predicted_column], nbins = bins, method = 'l', labels=c(1:bins))
  
  # Contar la cantidad de casos positivos por grupo
  positive_class <- dataset %>% filter(!!sym(class_column)==positive_value) %>% group_by(group) %>% count()
  
  # Obtener la media de las predicciones por grupo
  HL_df <- dataset %>% group_by(group) %>% summarise(pred=mean(!!sym(predicted_column)), count=n()) %>%
            inner_join(.,positive_class) %>%
            mutate(freq=n/count)
  
  # Gráfico 
  HM_plot <- ggplot(HL_df, aes(x=pred, y=freq)) + geom_point(aes(size=n), color=color) +
                geom_text(aes(label=n),nudge_y = nudge_y)+
                geom_abline(slope = 1, intercept = 0, linetype='dashed') + 
                theme_bw() +
                labs(title='Hosmer-Lemeshow', size='Casos', x="Probabilidad Predicha", y="Frecuencia observada")
  return(HM_plot)
}

Hosmer_Lemeshow_plot(prediction_full, '.fitted', 'default', 10, 1) + labs(subtitle="Modelo completo")

Hosmer_Lemeshow_plot(prediction_bad, '.fitted', 'default', 10, 1, color = "firebrick", nudge_y = 0.003) + scale_x_continuous(limits = c(0.02,.07)) + scale_y_continuous(limits = c(.02,.07)) + labs(subtitle="Modelo malo")


# Hosmer_Lemeshow_plot(default, 'predicted_full', 'default', 10, 'Yes') + labs(subtitle="Modelo completo")
# 
# Hosmer_Lemeshow_plot(default, 'predicted_bad', 'default', 10, 'Yes', color = "firebrick", nudge_y = 0.003) + scale_x_continuous(limits = c(0.02,.07)) + scale_y_continuous(limits = c(.02,.07)) + labs(subtitle="Modelo malo")

```

¿Qué vemos en estos gráficos?

¿Para qué valores parece existir una sobreestimación de la probabilidad? ¿Para cuáles subestimación?

## Punto de corte

Hasta ahora hemos evaluado el modelo de manera general, pero el resultado final del modelo debe consistir en asignar al individuo una clase predicha. En nuestro caso debemos establecer un punto de corte según el cual vamos a separar a los indivuos en quienes defaultean y quienes no.

Probamos varios puntos de corte y graficamos el accuracy, la sensibilidad y la especificidad para cada uno de ellos.

```{r}
# prediction_metrics <- function(cutoff, predicted_column, dataset=default){
#   
#   table <- dataset %>% mutate(predicted_class=if_else(!!sym(predicted_column)>cutoff, 'Yes', 'No') %>% as.factor())
#   
#   
#   cm <- confusionMatrix(table(table$default, table$predicted_class), positive = 'Yes') %>%
#     tidy() %>%
#     select(term, estimate) %>% 
#     filter(term %in% c('accuracy', 'sensitivity', 'specificity')) %>% 
#     mutate(cutoff=cutoff)
#   return(cm)
# }


prediction_metrics <- function(cutoff, predictions=prediction_full){
  table <- predictions %>% 
    mutate(predicted_class=if_else(.fitted>cutoff, 1, 0) %>% as.factor(),
           default= factor(default))
  
  confusionMatrix(table(table$default, table$predicted_class), positive = "1") %>%
    tidy() %>%
    select(term, estimate) %>%
    filter(term %in% c('accuracy', 'sensitivity', 'specificity')) %>%
    mutate(cutoff=cutoff)
  
}



cutoffs = seq(0.01,0.97,0.01)
logit_pred= map_dfr(cutoffs, prediction_metrics)%>% mutate(term=as.factor(term))

# logit_pred =map2_dfr(cutoffs, 'predicted_full', prediction_metrics) %>% mutate(term=as.factor(term))

ggplot(logit_pred, aes(cutoff,estimate, group=term, color=term)) + geom_line() + theme_bw() + labs(title= 'Accuracy, Sensitivity y Specificity', subtitle= 'Modelo completo')
```

¿Qué podemos observar en el gráfico?

¿Podemos definir un buen punto de corte? ¿Cuál sería?
